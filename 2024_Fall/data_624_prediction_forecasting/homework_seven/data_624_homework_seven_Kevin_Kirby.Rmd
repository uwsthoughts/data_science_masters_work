---
title: "DATA 624 Homework Seven - Linear Regression"
author: "Kevin Kirby"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
```

## Overview

This is homework seven of the Fall 2024 edition of DATA 624. The assignment covers questions 6.2 and 6.3 from the exercise section of chapter 6 in [Applied Predictive Modeling by Max Kuhn and Kjell Johnson](http://appliedpredictivemodeling.com/)

First, most of the requried libraries.

```{r libraries}
library(tidyverse)
library(elasticnet)
library(caret)
library(lars)
library(pls)
library(stats)
library(corrplot)
library(MASS)
library(robustbase)  
library(glmnet)
```

## 6.2 Predicting permeability 

This exercise consists of component questions A through F that are about developing permeability prediction models for a pharmaceutical company.

### A. Load library and data 

The first question just says to load the library for the book and the dataset used.

```{r load-data}
library(AppliedPredictiveModeling)
data(permeability)
```


### B. Filtering out predictors with NearZeroVar

The question states:
" The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the  molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package.  How many predictors are left for modeling?"

Answer: there are 388 predictors left after filtering out those with low frequencies.

```{r nzv-filter}
nzv_fingers <- preProcess(fingerprints, method = c("nzv")) |>
    predict(fingerprints)

prem_predict <- ncol(nzv_fingers)
prem_predict
```


### C. Preprocessing and PLS modeling

The question states:
"Split the data into a training and a test set, pre-process the data, and  tune a PLS model. How many latent variables are optimal and what is  the corresponding resampled estimate of R2?"

Answer:

There are 10 optimal latent variables and the corresponding resampled estimate of R2 is 0.36914.

```{r preprocess-pls}
nzv_perm_s <- cbind(nzv_fingers, permeability)

nzv_perm <- preProcess(nzv_perm_s, method = c("BoxCox", "center", "scale", "knnImpute")) |>
    predict(nzv_perm_s) 

part <- createDataPartition(nzv_perm[permeability], p = 0.75, list = FALSE)
nzv_perm_train <- nzv_perm[part,]
nzv_perm_test <- nzv_perm[-part,]

pls_control <- trainControl(method = "LOOCV")

pls_train <- train(permeability ~ .,
  data = nzv_perm_train,
  method = "pls",
  tuneLength = 20,
  trControl = pls_control)

pls_train

opt_lat_vars <- pls_train$bestTune$ncomp
rs_r2 <- max(pls_train$results$Rsquared)

cat("There are ", opt_lat_vars, " optimal latent variables and the corresponding resampled estimate of R2 is ",  rs_r2)
```


### D. Predict the response for the test set. What is the test set estimate of R2? 

Answer:

The test set estimate of R2 is 0.5593098

```{r response-predict}
predict(pls_train, nzv_perm_test) %>% 
  data.frame(pred = .,obs = nzv_perm_test[,"permeability"]) %>% 
  defaultSummary()

```

### E. Try building other models discussed in this chapter. Do any have better  predictive performance? 

Answer:

First model: robust linear regression

Results:
Samples: 102
Predictors: 388

The robust linear model yielded an RMSE of 0.9446 and an R² of 0.3589, which is lower than the previous model's R² of 0.5593, indicating that the model did not perform as well in terms of explained variance. The MAE of 0.7090 is slightly higher, suggesting less accuracy in predicting permeability compared to the earlier model. Overall, the previous model with a better R² and lower RMSE was more effective in capturing the relationship between predictors and permeability.

```{r lin-reg-swing, warning=FALSE}
nzv_fingers_lr <- preProcess(fingerprints, method = c("nzv")) |>
    predict(fingerprints)

nzv_perm_rl <- cbind(nzv_fingers_lr, permeability)

nzv_perm_lr <- preProcess(nzv_perm_rl, method = c("BoxCox", "center", "scale", "knnImpute")) |>
    predict(nzv_perm_rl) 

part_lr <- createDataPartition(nzv_perm_rl[permeability], p = 0.75, list = FALSE)
perm_train_rl <- nzv_perm_lr[part_lr,]
perm_test_rl <- nzv_perm_lr[-part_lr,]

control_lr <- trainControl(method = "LOOCV")

robust_lines <- train(permeability ~ .,
  data = perm_train_rl,
  method = "rlm",
  trControl = control_lr,
  preProcess = "pca"
  
)

robust_lines

predict(robust_lines, perm_test_rl) %>% 
  data.frame(pred = .,obs = perm_test_rl[,"permeability"]) %>% 
  defaultSummary()

```


Second model: Elastic net regression
Results:
Samples: 102
Predictors: 388

The Elastic Net model, using bootstrapped resampling with 10 repetitions, resulted in an RMSE of 0.6883, an R² of 0.5943, and an MAE of 0.5342. This model explored a grid of lambda and fraction parameters and selected the optimal values of lambda = 0.1 and fraction = 0.15 based on the smallest RMSE. The resampling process indicates that the Elastic Net model performs moderately well, with an R² value showing that it captures about 59.4% of the variance in the data. The relatively low MAE further indicates that the model is accurate in predicting permeability values with minimal error.

```{r elas-reg}

ctrl_enet <- trainControl(method = "boot", number = 10)

enet_g <- expand.grid(.lambda = c(0, 0.01, .1), .fraction = seq(.05, 1, length = 20))

enet_reg <- train(permeability ~ .,
  data = nzv_perm_train,
  method = "enet",
  tuneGrid = enet_g,
  trControl = ctrl_enet)

enet_reg

predict(enet_reg, nzv_perm_test) %>% 
  data.frame(pred = .,obs = nzv_perm_test[,"permeability"]) %>% 
  defaultSummary()

```


Overall conclusion:
When comparing the models overall, the Elastic Net model performed the best, with the highest R² (0.5943) and the lowest RMSE (0.6883). The robust linear model, with an R² of 0.3589 and RMSE of 0.9446, was less effective in explaining the variance and predicting permeability accurately compared to the Elastic Net.

### F. Would you recommend any of your models to replace the permeability laboratory experiment? 

Answer:
I wouldn’t recommend using any of the models trained here to replace lab experiments for permeability due to a lot of variability. Pharmaceutical companies need much tighter standards than the results here provide.


## 6.3 Modeling biological raw materials and their manufacturing process

This exercise consists of component questions A through F

### A. Load the data

```{r dara-load}
data(ChemicalManufacturingProcess)

```


### B. Imputing missing values

The question states:
"A small percentage of cells in the predictor set contain missing values. Use  an imputation function to fill in these missing values."

Answer:

I used median impute to insert the median value where a missing value is.

```{r miss-input}
resolve_miss <- preProcess(ChemicalManufacturingProcess, method = c("medianImpute")) |>
  predict(ChemicalManufacturingProcess)

```


### C. Preprocessing and tuning a model

The question states:
"Split the data into a training and a test set, pre-process the data, and  tune a model of your choice from this chapter. What is the optimal value  of the performance metric?"

Answer:

I will use Ridge Regression. This is a model that  which imposes a penalty on the coefficients based on the different lambdas that we will be training over.

First, preprocessing and setting up the train and test sets:

```{r pre-tune}
model_tuner <- preProcess(resolve_miss, method = c("center", "scale")) |>
  predict(resolve_miss)

ridge_slices <- createDataPartition(model_tuner$Yield, p = 0.75, list = FALSE)

ridge_train <- model_tuner[ridge_slices,]  
ridge_test <- model_tuner[-ridge_slices,]

cat("The training set has dimensions of: \n")
dim(ridge_train)

```


Finding optimal performance metric:
The optimal value of the performance metric RMSE is 1.831127, with a lambda of 0.1.

```{r ridge-predict}
ridge_control <- trainControl(method = "boot", number = 10)
ridge_roller <- expand.grid(.alpha = 0, .lambda = seq(0, .1, length = 15))

ridge_reg <- train(Yield ~ ., 
                   data = ridge_train,
                   method = "glmnet", 
                   tuneGrid = ridge_roller,
                   trControl = ridge_control)

ridge_reg
```

### D. Predicting test set response 

The question states:
Predict the response for the test set. What is the value of the performance  metric and how does this compare with the resampled performance metric  on the training set? 

Answer:
The RMSE for the test set is 0.7467, which is significantly lower than the resampled RMSE of 1.8311 on the training set. This suggests that the model generalizes well to the test set, showing better performance than during the resampling on the training set.

```{r reg-predict}

predict(ridge_reg, ridge_test) %>% 
  data.frame(pred = .,obs = ridge_test$Yield) %>% 
  defaultSummary()

```

### E. Assessing predictors value

The question states:
Which predictors are most important in the model you have trained? Do  either the biological or process predictors dominate the list? 

Answer:
There are 8 process predictors and 7 biological predictors in the top 15. Neither process dominates the list.

```{r predict-assess}
plot(varImp(ridge_reg), top = 15) 
  ggtitle("Top 15 predictors")
```


### F. Reviewing predictors and response relationships

The question states:
"Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future  runs of the manufacturing process?"

Answer:
The relationships between the top predictors and yield show some clear trends. For example, ManufacturingProcess32 and BiologicalMaterial06 have strong positive correlations, meaning boosting them could improve yield in future runs. On the other hand, ManufacturingProcess36 and ManufacturingProcess13 negatively impact yield, so controlling those might help reduce their drag on the process.

```{r predict-explore}
ridge_15 <- varImp(ridge_reg)$importance
ridge_15 <- as.data.frame(ridge_15)
ridge_predict <- rownames(ridge_15)[order(ridge_15[, 1], decreasing = TRUE)[1:15]]

for (rider in ridge_predict) {
  yields <- ggplot(ridge_train, aes(x = .data[[rider]], y = .data[["Yield"]])) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE, color = "skyblue") + 
    ggtitle(paste("Relationship between", rider, "and Yield")) +
    theme_minimal()
  
  print(yields)
}

ridge_corrs <- sapply(ridge_predict, function(x) cor(ridge_train[[x]], ridge_train$Yield, use = "complete.obs"))
ridges <- data.frame(Predictor = ridge_predict, Correlation = ridge_corrs)
ridges

```
