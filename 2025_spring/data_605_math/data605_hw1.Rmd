---
title: "DATA 605 Homework One"
author: "Kevin Kirby"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This is homework one for DATA 605: Computational mhemics at the CUNY School of Professional Studies. The assignment consists of four component questions.

First, some required libraries:
```{r libraries, message=FALSE}
library(ggplot2)
library(gganimate)
library(transformr)
library(tibble)
library(jpeg)
library(gridExtra)
library(pracma)
library(magick)
library(matrixStats)
library(irlba)
library(reshape2)

```


### 1.Geometric Transformion of ss Using matrix Multiplication

Question one states:
"Context: In computer graphics and data visualization, geometric transformions are fundamental. These transformions, such as translation, scaling, rotation, and reflection, can be applied to ss to manipulate their appearance.

Task: Create a simple s (like a square or triangle) using point plots in R. Implement R code to apply different transformions (scaling, rotation, reflection) to the s by left multiplying a transformion matrix by each of the point vectors. Demonstrate these transformions through animed plots.

Create a s: Define a simple s (e.g., a square) using a set of point coordinates.

Apply Transformions:

Scaling: Enlarge or shrink the s.
Rotation: Rotate the s by a given angle.
Reflection: Reflect the s across an axis.

Anime Transformions: Use a loop to incrementally change the transformion matrix and visualize the effect on the s over time.

Plot: Display the original s and its transformions in your compiled pdf.  Demonstrate the effect of the transformions with fixed images."

Answer:

```{r geom-transform}

square <- matrix(c(0, 0, 1, 0, 1, 1, 0, 1, 0, 0), ncol = 2, byrow = TRUE)
colnames(square) <- c("x", "y")

scale_bot <- function(sx, sy) matrix(c(sx, 0, 0, sy), nrow = 2)
chopper_blades <- function(theta) matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), nrow = 2)
mirrors <- function(axis) {
  if (axis == "x") return(matrix(c(1, 0, 0, -1), nrow = 2))
  if (axis == "y") return(matrix(c(-1, 0, 0, 1), nrow = 2))
}

s_shifter <- function(s, m) {
  t(m %*% t(s[, 1:2]))
}

clicks <- 30
hyps <- seq(0, pi / 2, length.out = clicks)
liz_frecs <- seq(1, 2, length.out = clicks)
holder <- list()

for (i in seq_len(clicks)) {
  blades <- s_shifter(square, chopper_blades(hyps[i]))
  frecs <- s_shifter(blades, scale_bot(liz_frecs[i], liz_frecs[i]))
  mirrored <- s_shifter(frecs, mirrors("x"))
  holder[[i]] <- tibble(x = mirrored[, 1], y = mirrored[, 2], frame = i)
}

active_math <- do.call(rbind, holder)

p <- ggplot(active_math, aes(x, y, group = frame)) +
  geom_polygon(fill = "blue", alpha = 0.5, color = "black") +
  coord_fixed() +
  theme_minimal() +
  transition_states(frame, transition_length = 2, state_length = 1)

```

```{r animate, echo=TRUE, eval=FALSE}
animate(p, fps = 10, duration = 5, width = 500, height = 500)
```

### 2Matrix Properties and Decomposition

Question two states:

"Proofs
* Prove that AB!= BA
* Prove that A^T A is always symmetric
* Prove that the determinant of A^T A is non-negative
* Singular Value Decomposition (SVD) and Image Compression

Context:  Every time you upload a photo to social media, algorithms often compress your image to reduce file size without significantly degrading quality. One of the key techniques used in this process is Singular Value Decomposition (SVD), which is another form of matrix factorization.

Task:  Write an R function that performs Singular Value Decomposition (SVD) on a grayscale image (which can be represented as a matrix). Use this decomposition to compress the image by keeping only the top k singular values and their corresponding vectors. Demonstrate the effect of different values of k on the compressed image's quality. You can choose any open-access grayscale image that is appropriate for a professional program.  

Instructions:

* **Read an Image**: Convert a grayscale image into a matrix.
* **Perform SVD**: Factorize the image matrix $\mathbf{A}$ into $U \Sigma V^T$ using R's built-in `svd()` function.
* **Compress the Image:** Reconstruct the image using only the top k singular values and vectors.
* **Visualize the Result:** Plot the original image alongside the compressed versions for various values of k (e.g., k = 5, 20, 50)."

Answer:

```{r image-compress}

gray_biker_path <- "/Users/uwsthoughts/Desktop/github_sync/data_science_masters_work/2025_spring/data_605_math/grayscale_photo.jpg"

A <- matrix(c(1, 2, 3, 4), nrow=2, ncol=2)
B <- matrix(c(0, 1, 1, 0), nrow=2, ncol=2)
AB <- A %*% B
BA <- B %*% A
cat("is equal: ", all(AB == BA), "\n")

A <- matrix(runif(9), nrow=3, ncol=3)
ATA <- t(A) %*% A
cat("is symmetric: ", all(ATA == t(ATA)), "\n")

A <- matrix(runif(9), nrow=3, ncol=3)
ATA <- t(A) %*% A
det_a <- det(ATA)
good_vibes <- det_a >= 0
cat("the determinant is ", det_a, " and the non-negative value is ", good_vibes)


gray_biker <- readJPEG(gray_biker_path)
gs_biker <- gray_biker[,,1]

svd_decomp <- svd(gs_biker)
U <- svd_decomp$u
S <- diag(svd_decomp$d)
V <- svd_decomp$v

k_values <- c(5, 20, 50)
img_cmp <- list()
for (k in k_values) {
  S_k <- S
  S_k[(k+1):nrow(S_k), (k+1):ncol(S_k)] <- 0
  comp_gb <- U %*% S_k %*% t(V)
  img_cmp[[paste0("k=", k)]] <- comp_gb
}

par(mfrow=c(1, length(k_values) + 1))
image(gs_biker, col=gray.colors(255), main="Original Image")
for (k in k_values) {
  image(img_cmp[[paste0("k=", k)]], col=gray.colors(255), main=paste("k=", k))
}

```


### 3 Matrix Rank, Properties, and Eigenspace
Question 3 states:

"Find the rank of the matrix $\mathbf{A}$. Explain what the rank tells us about the linear independence of the rows and columns of matrix $\mathbf{A}$. Identify if there are any linear dependencies among the rows or columns.

\[
A =
\begin{bmatrix}
2 & 4 & 1 & 3 \\
-2 & -3 & 4 & 1 \\
5 & 6 & 2 & 8 \\
-1 & -2 & 3 & 7
\end{bmatrix}
\]

#### Matrix Rank Boundaries

- Given an $m \times n$ matrix where $m > n$, determine the maximum and minimum possible rank, assuming that the matrix is non-zero.
- Prove that the rank of a matrix equals the dimension of its row space (or column space). Provide an example to illustrate the concept.

#### Rank and Row Reduction

- Determine the rank of matrix $\mathbf{B}$. Perform a row reduction on matrix $\mathbf{B}$ and describe how it helps in finding the rank. Discuss any special properties of matrix $\mathbf{B}$ (e.g., is it a rank-deficient matrix?).

\[
B =
\begin{bmatrix}
2 & 5 & 7 \\
4 & 10 & 14 \\
1 & 2.5 & 3.5
\end{bmatrix}
\]

- Find the eigenvalues and eigenvectors of the matrix $\mathbf{A}$. Write out the characteristic polynomial and show your solution step by step. After finding the eigenvalues and eigenvectors, verify that the eigenvectors are linearly independent. If they are not, explain why.

\[
A =
\begin{bmatrix}
3 & 1 & 2 \\
0 & 5 & 4 \\
0 & 0 & 2
\end{bmatrix}
\]

#### Diagonalization of Matrix

* Determine if matrix $\mathbf{A}$ can be diagonalized. If it can, find the diagonal matrix and the matrix of eigenvectors that diagonalizes $\mathbf{A}$.
* Discuss the geometric interpretation of the eigenvectors and eigenvalues in the context of transformations. For instance, how does matrix $\mathbf{A}$ stretch, shrink, or rotate vectors in $\mathbb{R}^3$?

Answer:

```{r eigen}
A_one <- matrix(c(2, 4, 1, 3, 
              -2, -3, 4, 1, 
               5, 6, 2, 8, 
              -1, -2, 3, 7), nrow=4, byrow=TRUE)
rank_a1 <- qr(A_one)$rank
cat("The rank of A one is: ", rank_a1, "\n")

B <- matrix(c(2, 5, 7,
              4, 10, 14,
              1, 2.5, 3.5), nrow=3, byrow=TRUE)
rank_b <- qr(B)$rank
cat("\n", "The rank of B is: ", rank_b, "\n")
rref_b <- rref(B)
cat("\n", "The row reduced form of B is: ",  "\n", rank_b, "\n")

A_two <- matrix(c(3, 1, 2, 
               0, 5, 4, 
               0, 0, 2), nrow=3, byrow=TRUE)

eig_a2 <- eigen(A_two)
cat("\n", "The eigenvalue of A two is: ",  "\n", eig_a2$values, "\n")
cat("The eigenvectors of A two is: ",  "\n", eig_a2$vectors, "\n")

is_free <- qr(eig_a2$vectors)$rank == ncol(A_two)
cat("\n", "The linear indenpenence of the eigenvectors is: ",  "\n", eig_a2$vectors, "\n")

if (is_free) {
  print("A_two is diagonalizable.")
  D <- diag(eig_a2$values)
  P <- eig_a2$vectors
  print("Diagonal matrix D:")
  print(D)
  print("Matrix P:")
  print(P)
} else {
  print("A_two is not diagonalizable.")
}

print("Geometric Interpretation: The eigenvalues of A_two determine how vectors are stretched or shrunk along their corresponding eigenvectors in R^3.")


```
The eigenvalues 5,3,2 indicate that matrix A2â€‹ scales along the directions of its eigenvectors. These eigenvectors have a basis of R^3. This means A2 is the result of scaling that would happen regardless and not due to rotation. 

### 4. Project: top_faces from the LFW (Labeled Faces in the Wild) Dataset
Question 4 states:

"Context: top_faces are a popular application of Principal Component Analysis (PCA) in computer vision. They are used for face recognition by finding the principal components (eigenvectors) of the covariance matrix of a set of facial images. These principal components represent the "top_faces" that can be combined to approximate any face in the dataset.

Task: Using the LFW (Labeled Faces in the Wild) dataset, build and visualize top_faces that account for 80% of the variability in the dataset. The LFW dataset is a well-known dataset containing thousands of labeled facial images, available for academic research.

#### Instructions

**Download the LFW Dataset:**
* The dataset can be accessed and downloaded using the lfw module from the sklearn library in Python or by manually downloading it from the LFW website.
* In this case, we'll use the lfw module from Python's sklearn library.

**Preprocess the Images:**
* Convert the images to grayscale and resize them to a smaller size (e.g., 64x64) to reduce computational complexity.
* Flatten each image into a vector.\

**Apply PCA:**
* Compute the PCA on the flattened images.
* Determine the number of principal components required to account for 80% of the variability.
*
**Visualize top_faces:**
* Visualize the first few top_faces (principal components) and discuss their significance.
* Reconstruct some images using the computed top_faces and compare them with the original images.

Answer:

I downloaded the photos to my computer and will now covert them to grayscale in smaller size:

```{r preprocess}
lfw_path <- "/Users/uwsthoughts/Desktop/archive/lfw-deepfunneled/lfw-deepfunneled"
lfw_pics <- list.files(lfw_path, pattern = "\\.jpg$", recursive = TRUE, full.names = TRUE)

lfw_process <- function(image_path) {
  tryCatch({
    img <- image_read(image_path) %>%
      image_convert(colorspace = "Gray") %>%
      image_resize("64x64")
    img_m <- as.numeric(as.matrix(image_data(img, channels = "gray")))

    return(img_m)
  }, error = function(e) rep(NA, 64 * 64)) # Ensure consistent dimensions
}

lfw_data <- lapply(lfw_pics, lfw_process)
matrix_i <- do.call(rbind, lfw_data)
dim(matrix_i)

```

**Applying PCA**

```{r pca}
vcs <- colVars(matrix_i, na.rm = TRUE)
vcs[is.na(vcs)] <- 0  

threshold <- 1e-3  
nc_cols <- vcs > threshold
matrix_if <- matrix_i[, nc_cols, drop = FALSE]

ic_pca <- prcomp_irlba(matrix_if, n = min(100, ncol(matrix_if)), center = TRUE, scale. = TRUE)
var_sum <- ic_pca$sdev^2 / sum(ic_pca$sdev^2)

components <- which(cumsum(var_sum) >= 0.80)[1]
components

```

**Visualize top_faces:**

```{r visualize}
top_eig <- 10 
top_plots <- lapply(1:top_eig, function(i) {
  img_m <- matrix(ic_pca$rotation[, i], nrow = 64, byrow = TRUE)
  img_df <- melt(img_m)
  
  ggplot(img_df, aes(Var2, Var1, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "black", high = "white", midpoint = 0) +
    theme_void() +
    ggtitle(paste("Eigenface", i))
})

grid.arrange(grobs = top_plots, ncol = 4)


```

Significance:
By using grayscale for the reconstruction, its naturally showing the edges of shapes prominently through the use of sharp contrast. The images show that an original color image can be dramatically reduced, to the virtual non existence in a math sense, and brought back to life with strong features in place. It shows the general prevailing theory that datasets can be reduced without destroying their meaning.


**Image Reconstruction and Comparison**

```{r rebuild}
start_im <- matrix_i[537, ]  # Pick any index
project_im <- predict(ic_pca, matrix(start_im, nrow = 1))

recon_img <- function(k) {
  reconstructed <- ic_pca$center + ic_pca$rotation[, 1:k] %*% project_im[1:k]
  img_matrix <- matrix(reconstructed, 64, 64) 
  img_matrix <- img_matrix[, ncol(img_matrix):1]
  return(img_matrix)
}

num_comps <- c(5, 20, 50, components)

recon_plots <- lapply(num_comps, function(k) {
  img_matrix <- recon_img(k)
  img_df <- melt(img_matrix)
  colnames(img_df) <- c("Var1", "Var2", "value")

  ggplot(img_df, aes(Var2, -Var1, fill = value)) +  # Flip y-axis
    geom_tile() +
    scale_fill_gradient2(low = "black", high = "white", midpoint = 0) +
    theme_void() +
    ggtitle(paste(k, "components"))
})

grid.arrange(grobs = recon_plots, ncol = 2)

```

I could not get this image reconstruction to work when trying to compare to an original photo. 