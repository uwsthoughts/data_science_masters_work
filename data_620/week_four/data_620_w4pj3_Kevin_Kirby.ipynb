{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b262c3e1",
   "metadata": {},
   "source": [
    "# DATA 620, Project 3: Name/Gender Classifier:\n",
    "## Author: Kevin Kirby\n",
    "\n",
    "I chose a decision tree classifier because they're known to perform better on name classifier tasks than Naive Bayes or Maximum Entroy Classifiers.\n",
    "\n",
    "First, import required libraries. Please note, I downloaded the `names` Corpus using the NTLK corpus pickker via my terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf24412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import DecisionTreeClassifier\n",
    "from nltk.classify import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a45f6",
   "metadata": {},
   "source": [
    "## Classifier Functions\n",
    "\n",
    "`name_features()`: defines what features I'm interested in. Things like the last letter, first letter, and whether the name ends in a vowel usually fall along gender lines. The use of the last two letters and name length is meant to make the model more interesting than what would come the single character features.\n",
    "\n",
    "`ng_classify()`: splits the Names corpus into male and feamle, along with the required test/dev-test/train breakouts provided in the assignment. `ng_decision_tree` creates the decision tree and trains it on the training set. The `entropy_cutoff` of 0.1 is meant to allow the tree to grow to the point where it's starting to split on rare occurences and begins to overfit. `support_cutoff` of 7 requires at least 7 occurences of a feature pattern before splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a838772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_features(name, feature_names):\n",
    "    features = {}\n",
    "    if 'last_letter' in feature_names:\n",
    "        features['last_letter'] = name[-1]\n",
    "    if 'first_letter' in feature_names:\n",
    "        features['first_letter'] = name[0]\n",
    "    if 'last_is_vowel' in feature_names:\n",
    "        features['last_is_vowel'] = name[-1].lower() in 'aeiou'\n",
    "    if 'last_two' in feature_names:\n",
    "        features['last_two'] = name[-2:]\n",
    "    if 'name_length' in feature_names:\n",
    "        features['name_length'] = len(name)\n",
    "    return features\n",
    "\n",
    "def ng_classfiy(feature_names):\n",
    "    labels = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                     [(name, 'female') for name in names.words('female.txt')])\n",
    "    random.shuffle(labels)\n",
    "\n",
    "    train_set = [(name_features(n, feature_names), g) for (n, g) in labels[:6900]]\n",
    "    devtest_set = [(name_features(n, feature_names), g) for (n, g) in labels[6900:7400]]\n",
    "    test_set = [(name_features(n, feature_names), g) for (n, g) in labels[7400:7900]]\n",
    "\n",
    "    ng_decision_tree = DecisionTreeClassifier.train(train_set, entropy_cutoff=0.1, support_cutoff=7)\n",
    "    dt_accuracy = accuracy(ng_decision_tree, devtest_set)\n",
    "    test_accuracy = accuracy(ng_decision_tree, test_set)\n",
    "    \n",
    "    return dt_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9251de",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "I went through three rounds of incrementally adding features to assess performance. For readability, I've organized it below into sections by test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0852ee",
   "metadata": {},
   "source": [
    "### First Test, Three Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b82d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of using three features:\n",
      " dev test accuracy: 0.788\n",
      " test accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "ngf_start = ['last_is_vowel', 'first_letter', 'last_letter']\n",
    "dev_start, test_start = ng_classfiy(ngf_start)\n",
    "\n",
    "print(\"Results of using three features:\\n dev test accuracy: {}\\n test accuracy: {}\".format(dev_start, test_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9458699",
   "metadata": {},
   "source": [
    "### Second Test, Four Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49dc0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of using four features:\n",
      " dev test accuracy: 0.786\n",
      " test accuracy: 0.768\n"
     ]
    }
   ],
   "source": [
    "ngf_second = ['last_is_vowel', 'first_letter', 'last_letter', 'last_two']\n",
    "dev_second, test_second = ng_classfiy(ngf_second)\n",
    "\n",
    "print(\"Results of using four features:\\n dev test accuracy: {}\\n test accuracy: {}\".format(dev_second, test_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6d490",
   "metadata": {},
   "source": [
    "### Final Test, Five Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b36f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of using all five features:\n",
      " dev test accuracy: 0.778\n",
      " test accuracy: 0.782\n"
     ]
    }
   ],
   "source": [
    "ngf_final = ['last_is_vowel', 'first_letter', 'last_letter', 'last_two', 'name_length']\n",
    "dev_final, test_final = ng_classfiy(ngf_final)\n",
    "\n",
    "print(\"Results of using all five features:\\n dev test accuracy: {}\\n test accuracy: {}\".format(dev_final, test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15eb70e",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "It's very interesting to see that three features performed the best overall. The dataset is pretty small in the grand scheme of the universe so this could be the result of:\n",
    "    * Too high of entropy and support cutoffs relative to overall data it draws from\n",
    "    * Additional features introduced noise that wasn't helpful to the model\n",
    "\n",
    "I expected better performance for the five features than the three features. However, I look back and realize this was a naive expectations. Less than 8,000 data points overall is really inconsequential and I should have known to expect better performane from a simpler model. Smaller datasets are vulnerable to overfitting, especially when test and dev are only 500 points. \n",
    "\n",
    "I'm impressed by the baseline performance, though. Close to 80% accuracy as a starting point is a good launch pad for further refinement with a larger dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "av-work-env",
   "language": "python",
   "name": "av-work-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
